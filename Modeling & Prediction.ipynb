{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95116a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05738507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['facility_name', 'age_group', 'gender', 'race', 'ethnicity',\n",
       "       'length_of_stay', 'type_of_admission', 'patient_disposition',\n",
       "       'discharge_year', 'ccsr_diagnosis_code', 'ccsr_diagnosis_description',\n",
       "       'apr_drg_code', 'apr_drg_description', 'apr_mdc_code',\n",
       "       'apr_mdc_description', 'apr_severity_of_illness_code',\n",
       "       'apr_severity_of_illness_description', 'apr_risk_of_mortality',\n",
       "       'apr_medical_surgical_description', 'payment_typology_1',\n",
       "       'emergency_department_indicator', 'total_charges', 'total_costs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\CodigosGithub\\Hospital_LOS_Prediction\\data\\hospital_cleaned.csv\")\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fc3408",
   "metadata": {},
   "source": [
    "## Target variable\n",
    "\n",
    "**length_of_stay**\n",
    "\n",
    "## Predictor candidates\n",
    "\n",
    "**Categorical:**\n",
    "\n",
    "- age_group\n",
    "- gender\n",
    "- race\n",
    "- ethnicity\n",
    "- type_of_admission\n",
    "- patient_disposition\n",
    "- ccsr_diagnosis_description\n",
    "- apr_severity_of_illness_description\n",
    "- apr_risk_of_mortality\n",
    "- apr_medical_surgical_description\n",
    "- payment_typology_1\n",
    "- emergency_department_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55113bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 100,000 records for tuning and testing\n",
    "df_sample = df.sample(n=min(100_000, len(df)), random_state=42)\n",
    "\n",
    "# Define features and target variable\n",
    "target = 'length_of_stay'\n",
    "categorical_features = [\n",
    "    'age_group', 'gender', 'race', 'ethnicity',\n",
    "    'type_of_admission', 'patient_disposition',\n",
    "    'ccsr_diagnosis_description', 'apr_severity_of_illness_description',\n",
    "    'apr_risk_of_mortality', 'apr_medical_surgical_description',\n",
    "    'payment_typology_1', 'emergency_department_indicator'\n",
    "]\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df_sample[categorical_features]\n",
    "y = df_sample[target]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c916c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation:\n",
      "Root Mean Squared Error (RMSE): 6.91 days\n",
      "Mean Absolute Error (MAE): 3.26 days\n",
      "R² Score: 0.3276\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('categorical', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "])\n",
    "\n",
    "# Define the XGBoost Regressor (GPU-enabled)\n",
    "xgb_model = XGBRegressor(\n",
    "    tree_method='hist',      # modern XGBoost method\n",
    "    device='cuda',           # use CUDA GPU\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine preprocessing and model in a pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', xgb_model)\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model evaluation:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} days\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f} days\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd79eba",
   "metadata": {},
   "source": [
    "These results demonstrate the advantage of using machine learning to predict hospital stays based on different variables. Now that we know this, let's find the model that generates the best results and optimize the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ae27f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "RMSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "R²",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "4b2ab974-1201-46fa-b190-362bf3b7864a",
       "rows": [
        [
         "0",
         "XGBoost (GPU)",
         "6.909995603877435",
         "3.2618110179901123",
         "0.32764822244644165"
        ],
        [
         "1",
         "Gradient Boosting",
         "7.035037853900603",
         "3.3928478573701795",
         "0.30309450351237033"
        ],
        [
         "2",
         "Linear Regression",
         "7.180825850679546",
         "3.583337113210265",
         "0.2739110938169007"
        ],
        [
         "3",
         "Random Forest",
         "7.37702877634745",
         "3.431733967731098",
         "0.2336909267831938"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost (GPU)</td>\n",
       "      <td>6.909996</td>\n",
       "      <td>3.261811</td>\n",
       "      <td>0.327648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>7.035038</td>\n",
       "      <td>3.392848</td>\n",
       "      <td>0.303095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>7.180826</td>\n",
       "      <td>3.583337</td>\n",
       "      <td>0.273911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>7.377029</td>\n",
       "      <td>3.431734</td>\n",
       "      <td>0.233691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model      RMSE       MAE        R²\n",
       "0      XGBoost (GPU)  6.909996  3.261811  0.327648\n",
       "1  Gradient Boosting  7.035038  3.392848  0.303095\n",
       "2  Linear Regression  7.180826  3.583337  0.273911\n",
       "3      Random Forest  7.377029  3.431734  0.233691"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost (GPU)\": XGBRegressor(tree_method='hist', device='cuda', n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through models and evaluate\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).sort_values(by='RMSE')\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e099b085",
   "metadata": {},
   "source": [
    "**Random Forest** and **XGBoost** showed the best results. Before training the model with all the data, we will do **hyperparameter tuning** with the sample of 100,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f872f0",
   "metadata": {},
   "source": [
    "# Hiperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d05ce43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "\n",
      "--- Final Evaluation on Sampled Test Set ---\n",
      "\n",
      "Random Forest Evaluation:\n",
      "RMSE: 6.9861 days\n",
      "MAE : 3.3720 days\n",
      "R²  : 0.3127\n",
      "XGBoost Evaluation:\n",
      "RMSE: 6.9216 days\n",
      "MAE : 3.2934 days\n",
      "R²  : 0.3254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(6.92158235572827), 3.2934014797210693, 0.3253914713859558)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Common preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# Random Forest Tuning\n",
    "# -----------------------------\n",
    "rf_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', rf_model)\n",
    "])\n",
    "\n",
    "rf_params = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [10, 20, 30],\n",
    "    'model__min_samples_split': [2, 5],\n",
    "    'model__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    rf_params,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train, y_train)\n",
    "rf_best_model = rf_search.best_estimator_\n",
    "\n",
    "# -----------------------------\n",
    "# XGBoost Tuning (GPU)\n",
    "# -----------------------------\n",
    "xgb_model = XGBRegressor(tree_method='hist', device='cuda', random_state=42)\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', xgb_model)\n",
    "])\n",
    "\n",
    "xgb_params = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [3, 6, 10],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'model__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb_pipeline,\n",
    "    xgb_params,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_train, y_train)\n",
    "xgb_best_model = xgb_search.best_estimator_\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate and Save Models\n",
    "# -----------------------------\n",
    "def evaluate_model(model, X_test, y_test, name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{name} Evaluation:\")\n",
    "    print(f\"RMSE: {rmse:.4f} days\")\n",
    "    print(f\"MAE : {mae:.4f} days\")\n",
    "    print(f\"R²  : {r2:.4f}\")\n",
    "    return rmse, mae, r2\n",
    "\n",
    "print(\"\\n--- Final Evaluation on Sampled Test Set ---\\n\")\n",
    "evaluate_model(rf_best_model, X_test, y_test, \"Random Forest\")\n",
    "evaluate_model(xgb_best_model, X_test, y_test, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9aabd398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for XGBoost:\n",
      "{'model__subsample': 1.0, 'model__n_estimators': 100, 'model__max_depth': 6, 'model__learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters for XGBoost:\")\n",
    "print(xgb_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbce126",
   "metadata": {},
   "source": [
    "The best model seems to be **XGBoost** with the following hyperparameters:\n",
    "\n",
    "- model_subsample = 1\n",
    "- model_n_estimators = 100\n",
    "- model_max_depth = 6\n",
    "- model_learning_rate = 0.1\n",
    "\n",
    "Now, I'm going to train this model with all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc742af",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf357fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training XGBoost model on full dataset...\n",
      "Model evaluation:\n",
      "Root Mean Squared Error (RMSE): 6.71 days\n",
      "Mean Absolute Error (MAE): 3.21 days\n",
      "R² Score: 0.3710\n"
     ]
    }
   ],
   "source": [
    "# Split data into features (X) and target (y)\n",
    "X = df[categorical_features]\n",
    "y = df[target]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "])\n",
    "\n",
    "# Best hyperparameters from tuning\n",
    "best_xgb = XGBRegressor(\n",
    "    tree_method='hist',\n",
    "    device='cuda',\n",
    "    n_estimators=100,          \n",
    "    max_depth=6,              \n",
    "    learning_rate=0.1,         \n",
    "    subsample=1.0,             \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Full pipeline\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', best_xgb)\n",
    "])\n",
    "\n",
    "# Train on full dataset\n",
    "print(\"🚀 Training XGBoost model on full dataset...\")\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model evaluation:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} days\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f} days\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46ce31",
   "metadata": {},
   "source": [
    "### Model Evaluation Summary\n",
    "\n",
    "The final **XGBoost** model was trained using the entire dataset with the best hyperparameters identified during tuning.\n",
    "\n",
    "**Performance on the validation set:**\n",
    "\n",
    "- Root Mean Squared Error (RMSE): **6.71 days**\n",
    "- Mean Absolute Error (MAE): **3.21 days**\n",
    "- R² Score: **0.3710**\n",
    "\n",
    "These results indicate that the model can explain approximately **37% of the variability** in patient length of stay (LOS), with an average error of **just over 3 days**.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Although the performance is acceptable for a first version, it will likely improve by:\n",
    "\n",
    "- Engineering new relevant features (e.g., diagnosis grouping, seasonal trends, bed availability)\n",
    "- Including external variables (e.g., hospital type, prior visits, socioeconomic indicators)\n",
    "- Trying advanced models (e.g., temporal models or ensemble stacking)\n",
    "\n",
    "For now, this version will be used to power the **bed assignment optimization model**, but continuous refinement is expected in future iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abbaa9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\CodigosGithub\\\\Hospital_LOS_Prediction\\\\models\\\\xgb_final_pipeline.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the pipeline (preprocessor + trained XGBoost)\n",
    "joblib.dump(final_pipeline, r\"C:\\CodigosGithub\\Hospital_LOS_Prediction\\models\\xgb_final_pipeline.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
